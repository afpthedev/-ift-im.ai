from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType, ArrayType, DoubleType
import os
import pickle
import numpy as np

# --- Mevcut Fonksiyonlar (Ã§oÄŸunlukla deÄŸiÅŸmedi) ---

def create_spark_session():
    """Hadoop ile entegre Spark oturumu oluÅŸturur"""
    spark = SparkSession.builder \
        .appName("TarÄ±m Tahmin - Spark ML") \
        .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
        .config("spark.yarn.appMasterEnv.PYSPARK_PYTHON", "/usr/local/bin/python") \
        .config("spark.executorEnv.PYSPARK_PYTHON", "/usr/local/bin/python") \
        .config("spark.driver.extraJavaOptions", "-Dlog4j.configuration=file:/opt/spark/conf/log4j.properties") \
        .config("spark.executor.extraJavaOptions", "-Dlog4j.configuration=file:/opt/spark/conf/log4j.properties") \
        .getOrCreate()
    spark.sparkContext.setLogLevel("WARN") # Daha az log Ã§Ä±ktÄ±sÄ± iÃ§in
    return spark

def load_data_from_hdfs(spark: SparkSession):
    """HDFS'ten tarÄ±m verilerini yÃ¼kler; baÅŸarÄ±sÄ±z olursa yerelden okumaya Ã§alÄ±ÅŸÄ±r."""
    hdfs_path = "hdfs://namenode:9000/user/hadoop/agri_predict/raw/fake_agricultural_data.csv"
    # spark_ml_pipeline.py dosyasÄ±nÄ±n konumuna gÃ¶re yerel yol ayarlandÄ±
    local_path = "../../data/raw/fake_agricultural_data.csv"

    try:
        print(f"HDFS'ten veri okunuyor: {hdfs_path}")
        df = spark.read.csv(hdfs_path, header=True, inferSchema=True)
        print("âœ… HDFS'ten veri baÅŸarÄ±yla okundu.")
        return df
    except Exception as e:
        print(f"âš ï¸ HDFS'ten veri okuma hatasÄ±: {e}")

    print(f"Yerel dosyadan veri okunuyor: {local_path}")
    if os.path.exists(local_path):
        try:
            df_local = spark.read.csv(local_path, header=True, inferSchema=True)
            print("âœ… Yerel dosyadan veri baÅŸarÄ±yla okundu.")
            return df_local
        except Exception as e_local:
            raise RuntimeError(f"Yerel dosyadan okuma esnasÄ±nda hata: {e_local}")
    else:
        raise FileNotFoundError(f"Veri dosyasÄ± ne HDFS'te ne de yerelde bulundu:\n"
                                f" - HDFS: {hdfs_path}\n"
                                f" - Yerel: {local_path}")

def prepare_data_for_prediction(df):
    """Tahmin iÃ§in veriyi hazÄ±rlar (VectorAssembler kullanÄ±r)"""
    feature_cols = ["soil_ph", "rainfall_mm", "temperature_celsius"]
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
    data = assembler.transform(df)
    return data

# --- Pickle Modeli Ä°Ã§in Yeni Fonksiyonlar ve MantÄ±k ---

def load_pickle_model(model_path):
    """Belirtilen yoldan bir pickle modelini yÃ¼kler."""
    try:
        with open(model_path, 'rb') as f:
            model = pickle.load(f)
        print(f"âœ… Pickle modeli baÅŸarÄ±yla yÃ¼klendi: {model_path}")
        return model
    except FileNotFoundError:
        print(f"âŒ Model dosyasÄ± bulunamadÄ±: {model_path}. LÃ¼tfen dosya yolunu kontrol edin.")
        raise
    except Exception as e:
        print(f"âŒ Pickle modeli yÃ¼klenirken hata oluÅŸtu: {e}. "
              f"Modelin doÄŸru bir pickle dosyasÄ± olduÄŸundan ve gerekli kÃ¼tÃ¼phanelerin (Ã¶rn. numpy, scikit-learn) kurulu olduÄŸundan emin olun.")
        raise

# Modelin yolu (spark_ml_pipeline.py dosyasÄ±ndan data/models/crop_predict_model.pkl'a gÃ¶re)
PICKLE_MODEL_PATH = "../../data/models/crop_predict_model.pkl"

# Spark driver'Ä±nda modelin bir kez yÃ¼klenmesini saÄŸlamak iÃ§in global bir deÄŸiÅŸken kullanÄ±yoruz.
# UDF'ler, closure'larÄ± otomatik olarak serileÅŸtirir ve iÅŸÃ§ilere gÃ¶nderir.
# EÄŸer model Ã§ok bÃ¼yÃ¼kse ve bu yÃ¶ntemle performans sorunlarÄ± yaÅŸarsanÄ±z,
# SparkContext.broadcast kullanmayÄ± dÃ¼ÅŸÃ¼nebilirsiniz.
loaded_sk_model = None
try:
    loaded_sk_model = load_pickle_model(PICKLE_MODEL_PATH)
except Exception as e:
    print(f"Uygulama baÅŸlatÄ±lÄ±rken pickle modeli yÃ¼klenemedi: {e}")
    # Model yÃ¼klenemezse uygulamanÄ±n devam etmemesi iÃ§in burada bir hata fÄ±rlatÄ±labilir.
    # raise # EÄŸer model yÃ¼klenemezse uygulamanÄ±n baÅŸlamasÄ±nÄ± engellemek iÃ§in bu satÄ±rÄ± etkinleÅŸtirin.

# UDF iÃ§in tahmin fonksiyonu
def predict_with_sk_model(features):
    """
    Scikit-learn modelini kullanarak tek bir Ã¶zellik vektÃ¶rÃ¼ Ã¼zerinde tahmin yapar.
    Bu fonksiyon bir Spark UDF'i olarak kullanÄ±lacaktÄ±r.
    """
    if loaded_sk_model is None:
        # Bu durum normalde olmamalÄ±, Ã§Ã¼nkÃ¼ modelin uygulama baÅŸlangÄ±cÄ±nda yÃ¼klenmesi beklenir.
        print("Hata: Scikit-learn modeli yÃ¼klenmemiÅŸ veya yÃ¼klenirken hata oluÅŸtu!")
        return -1 # Veya uygun bir hata deÄŸeri dÃ¶ndÃ¼rÃ¼n

    # Spark'Ä±n VectorAssembler'dan gelen yoÄŸun vektÃ¶rÃ¼ NumPy dizisine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    # EÄŸer seyrek vektÃ¶r geliyorsa 'features.toArray()' kullanÄ±n.
    # .reshape(1, -1) tek bir Ã¶rneÄŸi (row) bekleyen scikit-learn modelleri iÃ§in gereklidir.
    feature_array = np.array(features.toArray()).reshape(1, -1)
    prediction = loaded_sk_model.predict(feature_array)[0]
    return int(prediction) # Tahmin genellikle bir tamsayÄ±dÄ±r (sÄ±nÄ±f etiketi)

# UDF'i tanÄ±mla
# IntegerType, tahmin Ã§Ä±ktÄ±sÄ±nÄ±n tÃ¼rÃ¼nÃ¼ belirtir (province_id gibi).
predict_udf = udf(predict_with_sk_model, IntegerType())


def main():
    """Ana fonksiyon"""
    spark = create_spark_session()

    try:
        # Veriyi yÃ¼kle
        df = load_data_from_hdfs(spark)

        # Tahmin iÃ§in veriyi hazÄ±rla (features sÃ¼tunu oluÅŸtur)
        data_for_prediction = prepare_data_for_prediction(df)

        # Pickle modeli ile tahmin yap
        # 'features' sÃ¼tununu UDF'e girdi olarak veriyoruz
        predictions_df = data_for_prediction.withColumn("prediction", predict_udf(data_for_prediction["features"]))

        print("\n--- Tahmin SonuÃ§larÄ± (Ä°lk 5 satÄ±r) ---")
        # GerÃ§ek 'province_id' sÃ¼tunu varsa karÅŸÄ±laÅŸtÄ±rma iÃ§in gÃ¶sterilebilir.
        predictions_df.select("soil_ph", "rainfall_mm", "temperature_celsius", "province_id", "prediction").show(5)

        # Modeli deÄŸerlendirme (eÄŸer gerÃ§ek etiketler varsa)
        # Bu kÄ±sÄ±m, province_id'nin gerÃ§ek etiket olduÄŸunu varsayar.
        # EÄŸer sadece tahmin yapÄ±lÄ±yorsa bu kÄ±sma gerek yoktur.
        print("\n--- Model DeÄŸerlendirme ---")
        evaluator = MulticlassClassificationEvaluator(
            labelCol="province_id", predictionCol="prediction", metricName="accuracy")
        accuracy = evaluator.evaluate(predictions_df)
        print(f"Model doÄŸruluÄŸu (pickle modeli): {accuracy:.4f}")

        print("\nPickle modeli ile tahmin ve deÄŸerlendirme baÅŸarÄ±yla tamamlandÄ±.")

    except Exception as e:
        print(f"\nğŸš¨ Uygulama sÄ±rasÄ±nda genel bir hata oluÅŸtu: {e}")
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
